{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO: Stackoverflow dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CURARE view model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/Ge/anaconda2/lib/python2.7/site-packages (2.4.0)\n",
      "Requirement already satisfied: py4j==0.10.7 in /Users/Ge/anaconda2/lib/python2.7/site-packages (from pyspark) (0.10.7)\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/12/8c44eabb501e2bc0aec0dd152b328074d98a50968d3a02be28f6037f0c6a/jsonpickle-1.1-py2.py3-none-any.whl\n",
      "Installing collected packages: jsonpickle\n",
      "Successfully installed jsonpickle-1.1\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named statistics",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5e7896e0b259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named statistics"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os \n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import matplotlib\n",
    "import datetime as dt\n",
    "import statistics\n",
    "from scipy import stats\n",
    "import nltk\n",
    "import ipywidgets as widgets\n",
    "import pandas\n",
    "from collections import Counter\n",
    "from numpy import NaN\n",
    "import urllib.request\n",
    "import shutil\n",
    "from os import listdir\n",
    "import zipfile\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model class creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../libs/viewModel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to retrive attribute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getType (filePath):            \n",
    "        \n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath) \n",
    "    return df.dtypes # list N-tuple <attribute, data type>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHist (filePath):            \n",
    "        \n",
    "    #histList = []\n",
    "\n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath) \n",
    "    \n",
    "    for x in range(0,len(df.columns)):  \n",
    "\n",
    "        column_sel = df.select((df.columns[x])).collect()\n",
    "        column_name = df.dtypes[x][0]\n",
    "        column_type = df.dtypes[x][1]\n",
    "        column_array = np.array(column_sel)                                    \n",
    "        \n",
    "        if column_type == \"int\":\n",
    "            \n",
    "            # Replace None values for '0' to avoid error\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == None:\n",
    "                    column_array[n] = 0            \n",
    "                        \n",
    "            _hist = plt.hist(column_array) \n",
    "            plt.xlabel(column_name)\n",
    "            plt.ylabel(\"frequency\")\n",
    "            plt.show() \n",
    "            #histList.append(_hist) \n",
    "                                \n",
    "        #elif column_type == \"string\" or column_type == \"boolean\" or column_type == \"timestamp\":  \n",
    "        elif column_type == \"boolean\":  \n",
    "            \n",
    "            #if column_type == \"timestamp\":\n",
    "            #    for n, i in enumerate(column_array):\n",
    "            #        item = str(i)\n",
    "            #        column_array[n] = item\n",
    "            \n",
    "            dataf = pandas.DataFrame.from_dict(Counter(column_sel), orient='index')\n",
    "            p = dataf.plot(kind='bar', rot=90)\n",
    "            p.set_xlabel(column_name)\n",
    "            p.set_ylabel(\"frequency\")   \n",
    "            plt.show()            \n",
    "            #histList.append(dataf) \n",
    "            \n",
    "        else:\n",
    "            None\n",
    "            #histList.append(None)                        \n",
    "        \n",
    "    #return histList\n",
    "\n",
    "#getHist(\"/test/stackexchange/release_09-10-18/csv/Badges.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get number of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNull (filePath):            \n",
    "        \n",
    "    nullList = []\n",
    "    \n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath) \n",
    "\n",
    "    for x in range(0,len(df.columns)):   \n",
    "        \n",
    "        column_sel = df.select((df.columns[x])).collect()\n",
    "        column_array = np.array(column_sel)\n",
    "        column_par = sc.parallelize(column_array) # an RDD of arrays                                                   \n",
    "        \n",
    "        nulls = column_par.filter(lambda s: None in s).count() \n",
    "        nullList.append(nulls)        \n",
    "                \n",
    "    return nullList\n",
    "\n",
    "#getNull(\"/test/stackexchange/release_09-10-18/csv/Tags.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get number of absent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAbsent (filePath):            \n",
    "        \n",
    "    absentList = []\n",
    "    \n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath) \n",
    "        \n",
    "    for x in range(0,len(df.columns)):   \n",
    "        \n",
    "        column_sel = df.select((df.columns[x])).collect()\n",
    "        column_array = np.array(column_sel)        \n",
    "        column_par = sc.parallelize(column_array) # an RDD of arrays               \n",
    "        \n",
    "        emptys = column_par.filter(lambda s: \"''\" in s).count() \n",
    "        absentList.append(emptys)\n",
    "                \n",
    "    return absentList   \n",
    "\n",
    "#getAbsent(\"/test/stackexchange/release_09-10-18/csv/Tags - Copy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get min value per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMinVal (filePath):                    \n",
    "        \n",
    "    minValList = []    \n",
    "\n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath)       \n",
    "\n",
    "    for x in range(0,len(df.columns)): \n",
    "                      \n",
    "        column_sel = df.select((df.columns[x])).collect()\n",
    "        column_type = df.dtypes[x][1]\n",
    "        column_array = np.array(column_sel)             \n",
    "        \n",
    "        if column_type == \"int\":  \n",
    "            \n",
    "            # Replace None values for '0' to avoid error\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == None:\n",
    "                    column_array[n] = 0\n",
    "                               \n",
    "            column_c = sc.parallelize(column_array) # an RDD of arrays\n",
    "            summary = Statistics.colStats(column_c)  \n",
    "            minValList.append(int(summary.min()[0]))         \n",
    "            \n",
    "        elif column_type == \"boolean\":\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == False:\n",
    "                    column_array[n] = 0\n",
    "                elif i == True:\n",
    "                    column_array[n] = 1\n",
    "                    \n",
    "            column_c = sc.parallelize(column_array) # an RDD of arrays\n",
    "            summary = Statistics.colStats(column_c)  \n",
    "            minValList.append(int(summary.min()[0])) \n",
    "            \n",
    "        elif (column_type == \"string\" or column_type == \"timestamp\") and column_type is not dt.datetime: \n",
    "                                     \n",
    "            # Convert boolean to string\n",
    "            if column_type == \"timestamp\":                \n",
    "                for n, i in enumerate(column_array):\n",
    "                    item = str(i)\n",
    "                    column_array[n] = item\n",
    "                                                    \n",
    "            wList = []\n",
    "            fnpC = column_array.flatten()            \n",
    "                      \n",
    "            # get string lenght and creates a list\n",
    "            for i in fnpC:\n",
    "                if i is not None:\n",
    "                    wList.append(len(i))\n",
    "                elif i is None:\n",
    "                    wList.append(0)\n",
    "                else:\n",
    "                    wList.append(i)                                                                                                                           \n",
    "                                                                                                                           \n",
    "            minValList.append(min(wList))  \n",
    "            \n",
    "        else:\n",
    "            minValList.append(None)        \n",
    "        \n",
    "    return minValList\n",
    "\n",
    "#getMinVal(\"/test/stackexchange/release_09-10-18/csv/Badges - Copy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get max value per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMaxVal (filePath):                    \n",
    "        \n",
    "    maxValList = []\n",
    "\n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath)       \n",
    "\n",
    "    for x in range(0,len(df.columns)): \n",
    "                      \n",
    "        column_sel = df.select((df.columns[x])).collect()\n",
    "        column_type = df.dtypes[x][1]\n",
    "        column_array = np.array(column_sel)             \n",
    "        \n",
    "        if column_type == \"int\":  \n",
    "            \n",
    "            # Replace None values for '0' to avoid error\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == None:\n",
    "                    column_array[n] = 0\n",
    "                               \n",
    "            column_c = sc.parallelize(column_array) # an RDD of arrays\n",
    "            summary = Statistics.colStats(column_c)  \n",
    "            maxValList.append(int(summary.max()[0]))         \n",
    "            \n",
    "        elif column_type == \"boolean\":\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == False:\n",
    "                    column_array[n] = 0\n",
    "                elif i == True:\n",
    "                    column_array[n] = 1\n",
    "                    \n",
    "            column_c = sc.parallelize(column_array) # an RDD of arrays\n",
    "            summary = Statistics.colStats(column_c)  \n",
    "            maxValList.append(int(summary.max()[0])) \n",
    "            \n",
    "        elif (column_type == \"string\" or column_type == \"timestamp\") and column_type is not dt.datetime: \n",
    "                                     \n",
    "            # Convert boolean to string\n",
    "            if column_type == \"timestamp\":                \n",
    "                for n, i in enumerate(column_array):\n",
    "                    item = str(i)                                                            \n",
    "                    column_array[n] = item\n",
    "                                                    \n",
    "            wList = []\n",
    "            fnpC = column_array.flatten()            \n",
    "                      \n",
    "            # get string lenght and creates a list\n",
    "            for i in fnpC:\n",
    "                if i is not None:\n",
    "                    wList.append(len(i))\n",
    "                elif i is None:\n",
    "                    wList.append(0)\n",
    "                else:\n",
    "                    wList.append(i)                                                                                                                           \n",
    "                                                                                                                           \n",
    "            maxValList.append(max(wList))  \n",
    "            \n",
    "        else:\n",
    "            maxValList.append(None)        \n",
    "        \n",
    "    return maxValList\n",
    "\n",
    "#getMaxVal(\"/test/stackexchange/release_09-10-18/csv/Badges - Copy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get mean value per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMeanVal (filePath):                    \n",
    "        \n",
    "    meanValList = []\n",
    "\n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath)       \n",
    "\n",
    "    for x in range(0,len(df.columns)): \n",
    "                      \n",
    "        column_sel = df.select((df.columns[x])).collect()\n",
    "        column_type = df.dtypes[x][1]\n",
    "        column_array = np.array(column_sel)             \n",
    "        \n",
    "        if column_type == \"int\":      \n",
    "            \n",
    "            # Replace None values for '0' to avoid error\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == None:\n",
    "                    column_array[n] = 0\n",
    "                               \n",
    "            column_c = sc.parallelize(column_array) # an RDD of arrays\n",
    "            summary = Statistics.colStats(column_c)  \n",
    "            meanValList.append(int(summary.mean()[0]))         \n",
    "            \n",
    "        elif column_type == \"boolean\":\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == False:\n",
    "                    column_array[n] = 0\n",
    "                elif i == True:\n",
    "                    column_array[n] = 1\n",
    "                    \n",
    "            column_c = sc.parallelize(column_array) # an RDD of arrays\n",
    "            summary = Statistics.colStats(column_c)  \n",
    "            meanValList.append(int(summary.mean()[0])) \n",
    "            \n",
    "        elif (column_type == \"string\" or column_type == \"timestamp\") and column_type is not dt.datetime: \n",
    "                                     \n",
    "            # Convert boolean to string\n",
    "            if column_type == \"timestamp\":                \n",
    "                for n, i in enumerate(column_array):\n",
    "                    item = str(i)\n",
    "                    column_array[n] = item\n",
    "                                                    \n",
    "            wList = []\n",
    "            fnpC = column_array.flatten()            \n",
    "                      \n",
    "            # get string lenght and creates a list\n",
    "            sumL = 0\n",
    "            for i in fnpC:\n",
    "                if i is not None:\n",
    "                    wList.append(len(i))\n",
    "                    sumL = sumL + len(i)\n",
    "                elif i is None:\n",
    "                    wList.append(0)\n",
    "                else:\n",
    "                    wList.append(i)                     \n",
    "                                                                                                                           \n",
    "            meanValList.append(sumL/len(wList))  \n",
    "            \n",
    "        else:\n",
    "            meanValList.append(None)        \n",
    "        \n",
    "    return meanValList\n",
    "\n",
    "    #getMeanVal(\"/test/stackexchange/release_09-10-18/csv/Badges - Copy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get median value per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMedianVal (filePath):                    \n",
    "        \n",
    "    medianValList = []\n",
    "    \n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath)       \n",
    "\n",
    "    for x in range(0,len(df.columns)): \n",
    "                      \n",
    "        column_sel = df.select((df.columns[x])).collect()\n",
    "        column_type = df.dtypes[x][1]\n",
    "        column_array = np.array(column_sel)             \n",
    "        \n",
    "        if column_type == \"int\":             \n",
    "            \n",
    "            # Replace None values for '0' to avoid error\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == None:\n",
    "                    column_array[n] = 0\n",
    "                               \n",
    "            m = int(statistics.median(column_array)[0])\n",
    "            medianValList.append(m)\n",
    "            \n",
    "        elif column_type == \"boolean\":\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == False:\n",
    "                    column_array[n] = 0\n",
    "                elif i == True:\n",
    "                    column_array[n] = 1\n",
    "                                \n",
    "            m = int(statistics.median(column_array)[0])\n",
    "            medianValList.append(m)\n",
    "            \n",
    "        elif (column_type == \"string\" or column_type == \"timestamp\") and column_type is not dt.datetime: \n",
    "                                     \n",
    "            # Convert boolean to string\n",
    "            if column_type == \"timestamp\":                \n",
    "                for n, i in enumerate(column_array):\n",
    "                    item = str(i)\n",
    "                    column_array[n] = item\n",
    "                column_array = np.array(column_array, dtype=str)\n",
    "                        \n",
    "            wList = []\n",
    "            fnpC = column_array.flatten()            \n",
    "                      \n",
    "            # get string lenght and creates a list\n",
    "            for i in fnpC:\n",
    "                if i is not None:\n",
    "                    wList.append(len(i))\n",
    "                elif i is None:\n",
    "                    wList.append(0)                                                                                                                                                \n",
    "\n",
    "            m = int(statistics.median(wList))\n",
    "            medianValList.append(m)\n",
    "            \n",
    "        else:\n",
    "            medianValList.append(None)        \n",
    "        \n",
    "    return medianValList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get mode value per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModeVal (filePath):                    \n",
    "        \n",
    "    modeValList = []\n",
    "\n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath)   \n",
    "\n",
    "    for x in range(0,len(df.columns)):                 \n",
    "        \n",
    "        column_sel = df.select((df.columns[x])).collect()\n",
    "        column_type = df.dtypes[x][1]\n",
    "        column_array = np.array(column_sel)             \n",
    "        \n",
    "        if column_type == \"int\":                            \n",
    "            \n",
    "            # Replace None values for '0' to avoid error\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == None:\n",
    "                    column_array[n] = 0\n",
    "                    \n",
    "            m = int(stats.mode(column_array)[0][0][0])            \n",
    "            modeValList.append(m)\n",
    "            \n",
    "        elif column_type == \"boolean\":\n",
    "            for n, i in enumerate(column_array):\n",
    "                if i == False:\n",
    "                    column_array[n] = 0\n",
    "                elif i == True:\n",
    "                    column_array[n] = 1                                \n",
    "            \n",
    "            m = int(stats.mode(column_array)[0][0][0])            \n",
    "            modeValList.append(m)\n",
    "            \n",
    "        elif (column_type == \"string\" or column_type == \"timestamp\") and column_type is not dt.datetime: \n",
    "                                     \n",
    "            # Convert datetime to string\n",
    "            if column_type == \"timestamp\":                \n",
    "                for n, i in enumerate(column_array):\n",
    "                    item = str(i)                    \n",
    "                    column_array[n] = item\n",
    "                column_array = np.array(column_array, dtype=str)\n",
    "                \n",
    "            fnpC = column_array.flatten()                                                          \n",
    "            modeValList.append(nltk.FreqDist(fnpC).most_common(1)[0][0])                        \n",
    "            \n",
    "        else:\n",
    "            modeValList.append(None)                    \n",
    "        \n",
    "    return modeValList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get count per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCount (filePath):                    \n",
    "        \n",
    "    countList = []\n",
    "    df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filePath)                                                   \n",
    "    \n",
    "    return df.count()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read meta data file from atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import urllib.parse\n",
    "\n",
    "# Creates a client for the primary sandbox from cluster host cluster0-nlbcx.mongodb.net\n",
    "client = MongoClient(\"mongodb://adminUser:xpass@cluster0-shard-00-00-nlbcx.mongodb.net:27017/?ssl=true&replicaSet=Cluster0-shard-0&authSource=admin\")\n",
    "\n",
    "db = client.test \n",
    "db = client['stackoverflow-dump-db']\n",
    "collection = db['stackoverflow-stats-metadata-5']\n",
    "\n",
    "result = collection.find_one() # json file, can be browsed as a dictionary\n",
    "\n",
    "#print (result['releaseList'][0].keys())\n",
    "_size = result['size']; print (_size)\n",
    "_license = result['licence']; print (_license)\n",
    "_provider = result['provider']; print (_provider)\n",
    "_author = result['author']; print (_author)\n",
    "_description = result['description']; print (_description)\n",
    "_name = result['name']; print (_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create AttributeDescriptor objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releaseViewList = []\n",
    "releaseNum = 0\n",
    "\n",
    "url = 'https://archive.org/download/stackexchange/stats.stackexchange.com.7z'\n",
    "\n",
    "for r in os.listdir('../releases/'): \n",
    "    r = '../releases/' + r\n",
    "    attributesList = []\n",
    "    index = 0\n",
    "    for f in os.listdir(r): \n",
    "        f = r + \"/\" + f\n",
    "        print (f)\n",
    "\n",
    "        fullPath = f \n",
    "                                    # _id, name, _type, valueDistribution, nullValue, absentValue, \n",
    "                                    # minValue, maxValue, mean, median, mode, count):\n",
    "        attributes = AttributeDescriptor(url, f, getType(fullPath), getHist(fullPath), getNull(fullPath), \n",
    "                                         getAbsent(fullPath), getMinVal(fullPath), getMaxVal(fullPath), \n",
    "                                         getMeanVal(fullPath), getMedianVal(fullPath), getModeVal(fullPath), getCount(fullPath))    \n",
    "        attributesList.append(attributes)\n",
    "        attributesList[index].printInfo()\n",
    "        index += 1    \n",
    "    releaseNum += 1\n",
    "                                        # _id, version, publicationDate, size, _license, attributeDescList\n",
    "    releaseViewList.append(ReleaseView(url, None, None, _size, _license, attributesList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create View Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #_id, name, provider, author, description, releaseViewList\n",
    "view = View(url, _name, _provider, _author, _description, releaseViewList)\n",
    "view.printInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize view objet to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonpickle\n",
    "# <class '__main__.View'>\n",
    "viewJson = jsonpickle.encode(view) # <class 'str'>\n",
    "viewJson = json.loads(viewJson) # <class 'dict'>\n",
    "viewJson = json.dumps(viewJson, default=lambda x: x.__dict__, indent=3)\n",
    "viewJson = json.loads(viewJson) # <class 'dict'>\n",
    "\n",
    "import sys\n",
    "sizeJsons = sys.getsizeof(viewJson) # size of an object in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store view file in a MongoDB Atlas Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "import gridfs\n",
    "import pprint\n",
    "\n",
    "# Creates a client for the primary sandbox from cluster host cluster0-nlbcx.mongodb.net\n",
    "client = MongoClient(\"mongodb://adminUser:xpass@cluster0-shard-00-00-nlbcx.mongodb.net:27017/?ssl=true&replicaSet=Cluster0-shard-0&authSource=admin\")\n",
    "\n",
    "db = client['stackoverflow-dump-view']\n",
    "\n",
    "sizeJsons = sys.getsizeof(viewJson) # size of an object in bytes\n",
    "print (sizeJsons)\n",
    "#sizeJsons *= 100000\n",
    "#print (sizeJsons)\n",
    "              # 16793598 \n",
    "if sizeJsons >= 16000000: # if document is larger than 16MB GridFS has to be used    \n",
    "    #fs = gridfs.GridFS(db, collection = 'viewModel-view', disable_md5=False)\n",
    "    fs = gridfs.GridFS(db, collection = 'viewModel-view')\n",
    "    did = fs.put(str(viewJson), encoding='utf8')\n",
    "    print (did)\n",
    "else:\n",
    "    collection = db['viewModel-view']\n",
    "    #collection.insert_one(viewJson)  # this id can replace the url in datacollectionmodel class    \n",
    "    collection.insert(viewJson, check_keys = False)  # this id can replace the url in datacollectionmodel class    \n",
    "    pprint.pprint(collection.find_one())\n",
    "\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
